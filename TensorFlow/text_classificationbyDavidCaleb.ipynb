{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGNP0Ec1gH-U"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load IMDB dataset\n",
        "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "\n",
        "# Downloading IMDB dataset and extracting it\n",
        "dataset = tf.keras.utils.get_file(\"aclImdb_v1\", url,\n",
        "                                    untar=True, cache_dir='.',\n",
        "                                    cache_subdir='')\n",
        "\n",
        "# Creating path to dataset directory\n",
        "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')"
      ],
      "metadata": {
        "id": "Iay2OvEbgTrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Listing contents of dataset directory\n",
        "os.listdir(dataset_dir)"
      ],
      "metadata": {
        "id": "ruaslV3mhCJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating path to training data directory\n",
        "train_dir = os.path.join(dataset_dir, 'train')\n",
        "# Listing contents of training data directory\n",
        "os.listdir(train_dir)"
      ],
      "metadata": {
        "id": "ZMEcxwl1hP0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating path to a sample positive review file\n",
        "sample_file = os.path.join(train_dir, 'pos/1181_9.txt')\n",
        "\n",
        "# Opening and reading the sample review file\n",
        "with open(sample_file) as f:\n",
        "  print(f.read())"
      ],
      "metadata": {
        "id": "2vY9jk6JhWts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating path to directory containing unsupervised data\n",
        "remove_dir = os.path.join(train_dir, 'unsup')\n",
        "\n",
        "# Removing unsupervised data directory\n",
        "shutil.rmtree(remove_dir)"
      ],
      "metadata": {
        "id": "5CSpP6lSheoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting batch size for training\n",
        "batch_size = 32\n",
        "\n",
        "# Setting seed for reproducibility\n",
        "seed = 42\n",
        "\n",
        "# Creating raw training dataset from directory\n",
        "raw_train_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    # Specifying directory for training data\n",
        "    'aclImdb/train',\n",
        "    # Setting batch size\n",
        "    batch_size=batch_size,\n",
        "    # Splitting data for validation\n",
        "    validation_split=0.2,\n",
        "    # Specifying subset for training\n",
        "    subset='training',\n",
        "    # Setting seed for reproducibility\n",
        "    seed=seed)"
      ],
      "metadata": {
        "id": "CNoLt5Cfh5cI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for text_batch, label_batch in raw_train_ds.take(1):\n",
        "  for i in range(3):\n",
        "    print(\"Review\", text_batch.numpy()[i])\n",
        "    print(\"Label\", label_batch.numpy()[i])"
      ],
      "metadata": {
        "id": "oxt0G4aziJLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Label 0 corresponds to\", raw_train_ds.class_names[0])\n",
        "print(\"Label 1 corresponds to\", raw_train_ds.class_names[1])"
      ],
      "metadata": {
        "id": "oxoE2XoUiX6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating raw validation dataset from directory\n",
        "raw_val_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    # Specifying directory for training data\n",
        "    'aclImdb/train',\n",
        "    # Setting batch size\n",
        "    batch_size=batch_size,\n",
        "    # Splitting data for validation\n",
        "    validation_split=0.2,\n",
        "    # Specifying subset for validation\n",
        "    subset='validation',\n",
        "    # Setting seed for reproducibility\n",
        "    seed=seed)"
      ],
      "metadata": {
        "id": "3kD3wTiVijbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating raw test dataset from directory\n",
        "raw_test_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    # Specifying directory for test data\n",
        "    'aclImdb/test',\n",
        "    # Setting batch size\n",
        "    batch_size=batch_size)"
      ],
      "metadata": {
        "id": "IRXZ11dGi3Me"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare the dataset for training"
      ],
      "metadata": {
        "id": "f4xO2A-ii8xv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining custom standardization function\n",
        "def custom_standardization(input_data):\n",
        "  # Converting text to lowercase\n",
        "  lowercase = tf.strings.lower(input_data)\n",
        "  # Removing HTML tags\n",
        "  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
        "  # Removing punctuation\n",
        "  return tf.strings.regex_replace(stripped_html,'[%s]' % re.escape(string.punctuation),'')"
      ],
      "metadata": {
        "id": "x-YdSfU_i7Ee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting maximum number of features\n",
        "max_features = 10000\n",
        "\n",
        "# Setting sequence length\n",
        "sequence_length = 250\n",
        "\n",
        "# Creating TextVectorization layer\n",
        "vectorize_layer = layers.TextVectorization(\n",
        "    # Using custom standardization function\n",
        "    standardize=custom_standardization,\n",
        "    # Setting maximum number of tokens\n",
        "    max_tokens=max_features,\n",
        "    # Outputting integers\n",
        "    output_mode='int',\n",
        "    # Setting output sequence length\n",
        "    output_sequence_length=sequence_length)"
      ],
      "metadata": {
        "id": "irSMr8cPj94a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a text-only dataset (without labels), then call adapt\n",
        "# Extracting text from training dataset\n",
        "train_text = raw_train_ds.map(lambda x, y: x)\n",
        "# Adapting vectorization layer to training text\n",
        "vectorize_layer.adapt(train_text)"
      ],
      "metadata": {
        "id": "fSKGdd-Aj_l7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining function to vectorize text\n",
        "def vectorize_text(text, label):\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  return vectorize_layer(text), label"
      ],
      "metadata": {
        "id": "CKVXEok5kIwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# retrieve a batch (of 32 reviews and labels) from the dataset\n",
        "text_batch, label_batch = next(iter(raw_train_ds))\n",
        "first_review, first_label = text_batch[0], label_batch[0]\n",
        "print(\"Review\", first_review)\n",
        "print(\"Label\", raw_train_ds.class_names[first_label])\n",
        "print(\"Vectorized review\", vectorize_text(first_review, first_label))"
      ],
      "metadata": {
        "id": "j4uMjgnJkOBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"86 ---> \",vectorize_layer.get_vocabulary()[86]) # Printing word corresponding to index 86 in vocabulary\n",
        "print(\" 17 ---> \",vectorize_layer.get_vocabulary()[17]) # Printing word corresponding to index 17 in vocabulary\n",
        "print('Vocabulary size: {}'.format(len(vectorize_layer.get_vocabulary())))"
      ],
      "metadata": {
        "id": "l_gmcHUJkUzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = raw_train_ds.map(vectorize_text)\n",
        "val_ds = raw_val_ds.map(vectorize_text)\n",
        "test_ds = raw_test_ds.map(vectorize_text)"
      ],
      "metadata": {
        "id": "8RJTfxDfkphJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configure the dataset for performance"
      ],
      "metadata": {
        "id": "8bcvol4Fks6x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting AUTOTUNE parameter for dataset performance optimization\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "# Caching and prefetching training dataset for performance\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "# Caching and prefetching validation dataset for performance\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "# Caching and prefetching test dataset for performance\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "metadata": {
        "id": "PKSQ6_UMkweA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create the model"
      ],
      "metadata": {
        "id": "PJbA3mFQlEdI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 16"
      ],
      "metadata": {
        "id": "v_VyxqyClDrv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating sequential model\n",
        "model = tf.keras.Sequential([\n",
        "  # Adding embedding layer\n",
        "  layers.Embedding(max_features, embedding_dim),\n",
        "  # Adding dropout layer\n",
        "  layers.Dropout(0.2),\n",
        "  # Adding global average pooling 1D layer\n",
        "  layers.GlobalAveragePooling1D(),\n",
        "  # Adding dropout layer\n",
        "  layers.Dropout(0.2),\n",
        "  # Adding dense layer with sigmoid activation\n",
        "  layers.Dense(1, activation='sigmoid')])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "unq4ibCMlJwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss function and optimizer"
      ],
      "metadata": {
        "id": "wvM8nxW9lit-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compiling model with binary crossentropy loss\n",
        "model.compile(loss=losses.BinaryCrossentropy(),\n",
        "              optimizer='adam',  # Using Adam optimizer\n",
        "              metrics=[tf.metrics.BinaryAccuracy(threshold=0.5)])  # Using binary accuracy as evaluation metric"
      ],
      "metadata": {
        "id": "p8-G-kj9lk4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the model"
      ],
      "metadata": {
        "id": "dDSnsArLlxFV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10  # Setting number of epochs for training\n",
        "history = model.fit(  # Training the model\n",
        "    train_ds,  # Using training dataset\n",
        "    validation_data=val_ds,  # Using validation dataset for validation\n",
        "    epochs=epochs)  # Training for specified number of epochs"
      ],
      "metadata": {
        "id": "3rf7kwg5lvol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate the model\n"
      ],
      "metadata": {
        "id": "QoBabUJ3l0-m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how the model performs. Two values will be returned. Loss (a number which represents our error, lower values are better), and accuracy."
      ],
      "metadata": {
        "id": "l4sYFh8pmzs0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(test_ds)\n",
        "\n",
        "print(\"Loss: \", loss)\n",
        "print(\"Accuracy: \", accuracy)"
      ],
      "metadata": {
        "id": "3zO0n4obl4ZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a plot of accuracy and loss over time"
      ],
      "metadata": {
        "id": "dzV--LJum5uq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting history dictionary from training history\n",
        "history_dict = history.history\n",
        "# Displaying keys in history dictionary\n",
        "history_dict.keys()"
      ],
      "metadata": {
        "id": "C7G72gH7muPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting training accuracy\n",
        "acc = history_dict['binary_accuracy']\n",
        "# Extracting validation accuracy\n",
        "val_acc = history_dict['val_binary_accuracy']\n",
        "# Extracting training loss\n",
        "loss = history_dict['loss']\n",
        "# Extracting validation loss\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "# \"bo\" is for \"blue dot\"\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "# b is for \"solid blue line\"\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0YoaHSfUm_x5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mQhVnZMInJ4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this plot, the dots represent the training loss and accuracy, and the solid lines are the validation loss and accuracy."
      ],
      "metadata": {
        "id": "lvmyS5JInQTB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Export the model"
      ],
      "metadata": {
        "id": "lGQaTskUnqOg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "export_model = tf.keras.Sequential([\n",
        "  vectorize_layer,\n",
        "  model,\n",
        "  layers.Activation('sigmoid')\n",
        "])\n",
        "\n",
        "export_model.compile(\n",
        "    loss=losses.BinaryCrossentropy(from_logits=False), optimizer=\"adam\", metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Test it with `raw_test_ds`, which yields raw strings\n",
        "loss, accuracy = export_model.evaluate(raw_test_ds)\n",
        "print(accuracy)"
      ],
      "metadata": {
        "id": "4qPN_BOensxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference on new data"
      ],
      "metadata": {
        "id": "OvC0YmlGn53_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get predictions for new examples, you can simply call model.predict()."
      ],
      "metadata": {
        "id": "1QPaDM0hoLm2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "examples = tf.constant([\n",
        "    \"The movie was great!\",\n",
        "    \"The movie was okay\",\n",
        "    \"The movie was terrible...\"\n",
        "])\n",
        "export_model.predict(examples)"
      ],
      "metadata": {
        "id": "yfD-CR4bn32f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
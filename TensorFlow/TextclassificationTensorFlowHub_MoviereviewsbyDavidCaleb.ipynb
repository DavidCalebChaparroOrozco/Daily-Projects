{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_IalTI02Xz4M"
      },
      "outputs": [],
      "source": [
        "# Install necessary libraries\n",
        "!pip install tensorflow-hub\n",
        "!pip install tensorflow-datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "print(\"Version: \", tf.__version__)\n",
        "print(\"Eager mode: \", tf.executing_eagerly())\n",
        "print(\"Hub version: \", hub.__version__)\n",
        "print(\"GPU is\", \"available\" if tf.config.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")"
      ],
      "metadata": {
        "id": "jgoXxSztX-xv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download the IMB dataset"
      ],
      "metadata": {
        "id": "2VZp5aA-YK2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the training set into 60% and 40% to end up with 15,000 examples\n",
        "# for training, 10,000 examples for validation and 25,000 examples for testing.\n",
        "train_data, validation_data, test_data = tfds.load(\n",
        "    name=\"imdb_reviews\",\n",
        "    split=('train[:60%]', 'train[60%:]', 'test'),\n",
        "    as_supervised=True)"
      ],
      "metadata": {
        "id": "xoFD9Nt7YNh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explore the data\n",
        "In this example is a sentence representing the movie review and a corresponding label. The sentence is not preprocessed in any way. The label is an integer value of either 0 or 1, where 0 is negative review, and 1 is a positive review."
      ],
      "metadata": {
        "id": "2Rz_N6dgYfMF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First 10 examples\n",
        "train_examples_batch, train_labels_batch = next(iter(train_data.batch(10)))\n",
        "train_examples_batch"
      ],
      "metadata": {
        "id": "Gn0tjKlbYelG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels_batch"
      ],
      "metadata": {
        "id": "Yvq7HnEnY1jM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build the model\n",
        "The neural network is created by stacking layers- this requieres three main architectural decisions:\n",
        "- How to represent the text?\n",
        "- How many layers to use in the model?\n",
        "- How many _hidden units_ to use for each layer?\n",
        "\n",
        "The input data consists of sentences. The labels to predict are either 0 or 1.\n",
        "\n",
        "One way to represent the text is to convert sentences into embeddings vectors. Use a pre-trained text embedding as the first layer, which will have three advantages:\n",
        "- You don't have to worry about text preprocessing,\n",
        "- Benefit from transfer learning,\n",
        "- the embedding has a fixed size, so it's simpler to process.\n",
        "\n",
        "Let's first create a Keras layer that uses a TensorFlow Hub model to embed the sentences, and try it out a couple of input examples."
      ],
      "metadata": {
        "id": "EyySLlF6Y4OM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding = \"https://tfhub.dev/google/nnlm-en-dim50/2\"\n",
        "hub_layer = hub.KerasLayer(embedding, input_shape=[],\n",
        "                           dtype=tf.string, trainable=True)\n",
        "hub_layer(train_examples_batch[:3])"
      ],
      "metadata": {
        "id": "P-9Xc4wlZuIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's now build the full model\n",
        "model = tf.keras.Sequential()\n",
        "model.add(hub_layer)\n",
        "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(1))\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "X2fox0pvZx7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The layers are tacked sequentually to build the classifier:\n",
        "- The first layer is a TensorFlow Hub layer. This layer uses a pre-trained Saved Model to map a sentence into its embedding vector. The resulting dimensions are: (num_examples, embedding_dimension). For this NNLM model, the embedding_dimension is 50.\n",
        "- This fixed-length output vector is piped through a fully-connected (Dense) layer with 16 hidden units.\n",
        "- The last layer is densely connected with a single output node."
      ],
      "metadata": {
        "id": "jxLwXEQkZxGa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss function and optimizer\n",
        "The models needs a loss function and an optimizer for training. Since this is a binary classification problem and the model outputs logits (a single-unit layer with a linear activation), you'll use the binary_crossentropy loss function.\n",
        "Generally, binary_crossentropy is better for dealing with probabilitiesâ€”it measures the \"distance\" between probability distributions, or in our case, between the ground-truth distribution and the predictions."
      ],
      "metadata": {
        "id": "SioU7AORaPLP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure the model to use an optimizer and a loss function\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "s6jraPM6a_F-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the model\n",
        "Train the model for 10 epochs in mini-batches of 512 samples. This is 10 iterations over all samples in the x_train and y_train tensors. While training, monitor the model's loss and accuracy on the 10,000 samples from the validation set:"
      ],
      "metadata": {
        "id": "PHfdydVfbKu1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_data.shuffle(10000).batch(512),\n",
        "                    epochs=10,\n",
        "                    validation_data=validation_data.batch(512),\n",
        "                    verbose=1)\n"
      ],
      "metadata": {
        "id": "RcLbZCkjbJ4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate the model\n",
        "And let's see how the model performs. Two values will be returned. Loss (a number which represents our error, lower values are better), and accuracy."
      ],
      "metadata": {
        "id": "0DurNtcgbX7c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = model.evaluate(test_data.batch(512), verbose=2)\n",
        "\n",
        "for name, value in zip(model.metrics_names, results):\n",
        "  print(\"%s: %.3f\" % (name, value))\n"
      ],
      "metadata": {
        "id": "wCIOz_0WbdTk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# The pipelines:\n",
        "it simple to use any model from the Hub for inference on any language, computer vision, speech, and multimodal tasks.\n",
        "\n",
        "# Pipeline usage:\n",
        "While each task has an associated pipeline(), it is simpler to use the general pipeline() abstraction which contains all the task-specific pipelines. The pipeline() automatically loads a default model and a preprocessing class capable of inference for your task."
      ],
      "metadata": {
        "id": "94B-2qC9ixgX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "LFXwfMiUn-hf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7OGEuafiq9L"
      },
      "outputs": [],
      "source": [
        "# Start by creating a pipeline() and specify the inference task:\n",
        "from transformers import pipeline\n",
        "\n",
        "transcriber = pipeline(task=\"automatic-speech-recognition\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pass your input to the pipeline(). In the case of speech recognition, this is an audio input file:\n",
        "transcriber(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\")"
      ],
      "metadata": {
        "id": "0IPVBXdljQpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Whisper-large-V2 is a model from OpenAI, and was trained on close to 10x more data.\n",
        "transcriber = pipeline(model=\"openai/whisper-large-v2\")\n",
        "transcriber(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\")"
      ],
      "metadata": {
        "id": "LZWyL8xEkWVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This result looks more accurate."
      ],
      "metadata": {
        "id": "de0Tijtek0aZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If you have several inputs, you can pass your input as a list\n",
        "transcriber(\n",
        "    [\n",
        "        \"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\",\n",
        "        \"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/1.flac\",\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "2MHuc-3IlGGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parameters:\n",
        "The pipelines supports many parameters; some are task specific, and some are general to all pipelines. In general, you can specify parameters anywhere you want.\n",
        "\n",
        "```\n",
        "transcriber = pipeline(model=\"openai/whisper-large-v2\", my_parameter=1)\n",
        "\n",
        "out = transcriber(...)  # This will use `my_parameter=1`.\n",
        "out = transcriber(..., my_parameter=2)  # This will override and use `my_parameter=2`.\n",
        "out = transcriber(...)  # This will go back to using `my_parameter=1`.\n",
        "```\n",
        "\n",
        "# Device:\n",
        "If you use `device=n`, the pipeline automatically puts the model on the specified device. This will work regardless of whether you are using PyTorch or Tensorflow.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IS8LseOulr11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade accelerate"
      ],
      "metadata": {
        "id": "7lpxUKUqmeP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import pipeline\n",
        "\n",
        "# Check if CUDA is available and set the device accordingly\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "\n",
        "transcriber = pipeline(model=\"openai/whisper-large-v2\", device=device)"
      ],
      "metadata": {
        "id": "6AIhFDX6nu74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Batch size:\n",
        "This runs the pipeline on the 4 provided audio files, but it will pass them in batches of 2 to the model (which is on a GPU, where batching is more likely to help) without requiring any further code from you. The output should always match what you would have received without batching. It is only meant as a way to help you get more speed out of a pipeline.\n",
        "\n",
        "Pipelines can also alleviate some of the complexities of batching because, for some pipelines, a single item (like a long audio file) needs to be chunked into multiple parts to be processed by a model. The pipeline performs this chunk batching for you."
      ],
      "metadata": {
        "id": "wgPOD6G9m7Nq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transcriber = pipeline(model=\"openai/whisper-large-v2\", device=0, batch_size=2)\n",
        "audio_filenames = [f\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/{i}.flac\" for i in range(1, 5)]\n",
        "texts = transcriber(audio_filenames)"
      ],
      "metadata": {
        "id": "zlldJBhznZK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task specific parameters:\n",
        "All tasks provide task specific parameters which allow for additional flexibility and options to help you get your job done. For instance, the transformers.AutomaticSpeechRecognitionPipeline.call() method has a return_timestamps parameter which sounds promising for subtitling videos:"
      ],
      "metadata": {
        "id": "q3Gf9umqpjLK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transcriber = pipeline(model=\"openai/whisper-large-v2\", return_timestamps=True)\n",
        "transcriber(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\")"
      ],
      "metadata": {
        "id": "sChLV3Oso6Az"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are many parameters available for each task, so check out each taskâ€™s API reference to see what you can tinker with! For instance, the AutomaticSpeechRecognitionPipeline has a chunk_length_s parameter which is helpful for working on really long audio files (for example, subtitling entire movies or hour-long videos) that a model typically cannot handle on its own:"
      ],
      "metadata": {
        "id": "nxexDu8lqpU2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transcriber = pipeline(model=\"openai/whisper-large-v2\", chunk_length_s=30)\n",
        "transcriber(\"https://huggingface.co/datasets/reach-vb/random-audios/resolve/main/ted_60.wav\")"
      ],
      "metadata": {
        "id": "kUk-5C_NqUKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using pipelines on a dataset\n",
        "The pipeline can also run inference on a large dataset. The easiest way we recommend doing this is by using an iterator:"
      ],
      "metadata": {
        "id": "Rh_BcnrgqreV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data():\n",
        "    for i in range(1000):\n",
        "        yield f\"My example {i}\"\n",
        "\n",
        "\n",
        "pipe = pipeline(model=\"openai-community/gpt2\", device=0)\n",
        "generated_characters = 0\n",
        "for out in pipe(data()):\n",
        "    generated_characters += len(out[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "LHNxVwc7qx8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# KeyDataset is a util that will just output the item we're interested in.\n",
        "from transformers.pipelines.pt_utils import KeyDataset\n",
        "from datasets import load_dataset\n",
        "\n",
        "pipe = pipeline(model=\"hf-internal-testing/tiny-random-wav2vec2\", device=0)\n",
        "dataset = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation[:10]\")\n",
        "\n",
        "for out in pipe(KeyDataset(dataset, \"audio\")):\n",
        "    print(out)"
      ],
      "metadata": {
        "id": "MrHoApTrq78F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
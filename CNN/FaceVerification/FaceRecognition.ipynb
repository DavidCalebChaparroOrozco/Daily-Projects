{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2171faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential, load_model, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9817cbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "IMAGE_DIR = \"images\"\n",
    "# IMAGE_SIZE = (100, 100)\n",
    "# MobileNetV2 input size\n",
    "IMAGE_SIZE = (224, 224)\n",
    "MODEL_PATH = \"model/face_cnn_model.h5\"\n",
    "LABEL_MAP_PATH = \"model/label_map.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca39cae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load face database into dictionary\n",
    "def load_face_database(image_dir):\n",
    "    database = {\n",
    "        \"Kobe\": [],\n",
    "        \"Freeman\": [],\n",
    "        \"Caleb\": []\n",
    "    }\n",
    "    for file in os.listdir(image_dir):\n",
    "        path = os.path.join(image_dir, file)\n",
    "        if \"kobe\" in file.lower():\n",
    "            database[\"Kobe\"].append(path)\n",
    "        elif \"freeman\" in file.lower():\n",
    "            database[\"Freeman\"].append(path)\n",
    "        elif \"caleb\" in file.lower():\n",
    "            database[\"Caleb\"].append(path)\n",
    "    return database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9714f882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract face from image\n",
    "def extract_face(img):\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    for (x, y, w, h) in faces:\n",
    "        face = img[y:y+h, x:x+w]\n",
    "        return cv2.resize(face, IMAGE_SIZE)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c95c8dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess images\n",
    "def load_data(image_dir):\n",
    "    database = load_face_database(image_dir)\n",
    "    X, y = [], []\n",
    "    label_map = {name: idx for idx, name in enumerate(database)}\n",
    "    for label, paths in database.items():\n",
    "        for path in paths:\n",
    "            img = cv2.imread(path)\n",
    "            face = extract_face(img)\n",
    "            if face is not None:\n",
    "                X.append(face / 255.0)\n",
    "                y.append(label_map[label])\n",
    "    return np.array(X), to_categorical(y), label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "485ae996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "X, y, label_map = load_data(IMAGE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b098768c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save label_map to JSON\n",
    "os.makedirs(\"model\", exist_ok=True)\n",
    "with open(LABEL_MAP_PATH, \"w\") as file:\n",
    "    json.dump(label_map, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed26297c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation\n",
    "# datagen = ImageDataGenerator(\n",
    "#     rotation_range=40,\n",
    "#     width_shift_range=0.2,\n",
    "#     height_shift_range=0.2,\n",
    "#     shear_range=0.2,\n",
    "#     zoom_range=0.2,\n",
    "#     brightness_range=[0.6, 1.4],\n",
    "#     horizontal_flip=True,\n",
    "#     fill_mode='nearest'\n",
    "# )\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    brightness_range=[0.7, 1.3],\n",
    "    horizontal_flip=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "483079e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer Learning Model: MobileNetV2\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(len(label_map), activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "630affce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=base_model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57c86969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze base model\n",
    "for layer in base_model.layers[-20:]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ed2413",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a9b9636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 14s/step - accuracy: 1.0000 - loss: 0.5464"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:120: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 261ms/step - accuracy: 0.6296 - loss: 0.9929\n",
      "Epoch 2/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step - accuracy: 0.7778 - loss: 0.5695\n",
      "Epoch 3/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227ms/step - accuracy: 0.6296 - loss: 0.9562\n",
      "Epoch 4/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step - accuracy: 0.6296 - loss: 0.8885\n",
      "Epoch 5/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step - accuracy: 0.3704 - loss: 1.9073  \n",
      "Epoch 6/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step - accuracy: 0.2222 - loss: 3.5253  \n",
      "Epoch 7/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 212ms/step - accuracy: 0.6296 - loss: 0.7441\n",
      "Epoch 8/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.4213 - loss: 1.4725 \n",
      "Epoch 9/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step - accuracy: 0.3704 - loss: 1.9911  \n",
      "Epoch 10/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step - accuracy: 0.4444 - loss: 0.8813  \n",
      "Epoch 11/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.5370 - loss: 1.0450\n",
      "Epoch 12/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.4213 - loss: 1.4846 \n",
      "Epoch 13/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step - accuracy: 0.7037 - loss: 1.0371\n",
      "Epoch 14/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step - accuracy: 0.7037 - loss: 1.1607\n",
      "Epoch 15/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step - accuracy: 0.3704 - loss: 1.1567  \n",
      "Epoch 16/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step - accuracy: 0.3704 - loss: 1.3606  \n",
      "Epoch 17/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.6528 - loss: 0.8936 \n",
      "Epoch 18/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.4213 - loss: 1.1474 \n",
      "Epoch 19/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step - accuracy: 0.5185 - loss: 1.5537  \n",
      "Epoch 20/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step - accuracy: 0.3704 - loss: 1.4198  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained and saved.\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(datagen.flow(X, y, batch_size=8), epochs=20)\n",
    "model.save(MODEL_PATH)\n",
    "print(\"Model trained and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "081eb9b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# Load trained model and label map\n",
    "model = load_model(MODEL_PATH)\n",
    "with open(LABEL_MAP_PATH, \"r\") as f:\n",
    "    label_map = json.load(f)\n",
    "# invert to {0: 'Name'}\n",
    "label_map = {v: k for k, v in label_map.items()}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b30a30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press 'q' to quit...\n"
     ]
    }
   ],
   "source": [
    "# Real-time recognition\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# threshold = 0.7\n",
    "print(\"Press 'q' to quit...\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    faces = face_cascade.detectMultiScale(frame, 1.3, 5)\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        roi = frame[y:y+h, x:x+w]\n",
    "        face = cv2.resize(roi, IMAGE_SIZE)\n",
    "        face = np.expand_dims(face / 255.0, axis=0)\n",
    "\n",
    "        prediction = model.predict(face, verbose=0)[0]\n",
    "        label_index = np.argmax(prediction)\n",
    "        confidence = prediction[label_index]\n",
    "        name = label_map[label_index] #if confidence >= threshold else \"Unknown\"\n",
    "\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "        cv2.putText(frame, f\"{name} ({confidence:.2f})\", (x, y - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "\n",
    "    cv2.imshow(\"Face Recognition - MobileNetV2\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8816c134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Define CNN model\n",
    "# model = Sequential([\n",
    "#     Conv2D(32, (3,3), activation='relu', input_shape=(100,100,3)),\n",
    "#     MaxPooling2D(2,2),\n",
    "#     Conv2D(64, (3,3), activation='relu'),\n",
    "#     MaxPooling2D(2,2),\n",
    "#     Conv2D(128, (3,3), activation='relu'),\n",
    "#     MaxPooling2D(2,2),\n",
    "#     Flatten(),\n",
    "#     Dense(128, activation='relu'),\n",
    "#     # Reduce overfitting\n",
    "#     Dropout(0.5),  \n",
    "#     Dense(len(label_map), activation='softmax')\n",
    "# ])\n",
    "\n",
    "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e9b2298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train model\n",
    "# model.fit(datagen.flow(X, y, batch_size=8), epochs=50)\n",
    "# model.save(MODEL_PATH)\n",
    "# print(\"Model trained and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a14697d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reload model and label_map for prediction\n",
    "# with open(LABEL_MAP_PATH, \"r\") as file:\n",
    "#     label_map = json.load(file)\n",
    "\n",
    "# # Convert to {0: \"Kobe\", 1: \"Freeman\", 2: \"Caleb\"}\n",
    "# label_map = {v: k for k, v in label_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18267ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load OpenCV face detector\n",
    "# face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "# cap = cv2.VideoCapture(0)\n",
    "# print(\"Press 'q' to Quit.\")\n",
    "\n",
    "# while True:\n",
    "#     ret, frame = cap.read()\n",
    "#     if not ret:\n",
    "#         break\n",
    "\n",
    "#     faces = face_cascade.detectMultiScale(frame, 1.3, 5)\n",
    "\n",
    "#     for (x, y, w, h) in faces:\n",
    "#         roi = frame[y:y+h, x:x+w]\n",
    "#         img = cv2.resize(roi, IMAGE_SIZE)\n",
    "#         img = np.expand_dims(img / 255.0, axis=0)\n",
    "\n",
    "#         prediction = model.predict(img, verbose=0)[0]\n",
    "#         label_index = np.argmax(prediction)\n",
    "#         confidence = prediction[label_index]\n",
    "#         name = label_map[label_index]\n",
    "\n",
    "#         threshold = 0.7\n",
    "#         if confidence < threshold:\n",
    "#             name = \"Unknown\"\n",
    "\n",
    "#         cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "#         # text = f\"{name} ({confidence:.2f})\"\n",
    "#         # cv2.putText(frame, text, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "#         cv2.putText(frame, f\"{name} ({confidence:.2f})\", (x, y - 10),\n",
    "#                     cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "#     cv2.imshow(\"Face Recognition\", frame)\n",
    "#     if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "#         break\n",
    "\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

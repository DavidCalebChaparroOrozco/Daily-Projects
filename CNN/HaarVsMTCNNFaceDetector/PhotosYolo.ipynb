{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5bfe826",
   "metadata": {},
   "source": [
    "# Working with YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7a5cb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import fitz\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# YOLO\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69f85ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "input_dir = Path(\"photo/images\")\n",
    "pdf_dir = Path(\"pdfs\")\n",
    "output_dir_pose = Path(\"face_detected_YOLO\")\n",
    "cropped_dir_pose = Path(\"cropped_faces_YOLO\")\n",
    "pdf_cropped_dir = Path(\"pdf_cropped_faces_YOLO\")\n",
    "\n",
    "output_dir_pose.mkdir(parents=True, exist_ok=True)\n",
    "cropped_dir_pose.mkdir(parents=True, exist_ok=True)\n",
    "pdf_cropped_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c585974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n-pose.pt to 'yolov8n-pose.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6.52M/6.52M [00:02<00:00, 3.37MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Load YOLOv5 Pose model\n",
    "# model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
    "model = YOLO('yolov8n-pose.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ca93772",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_faces_yolo_pose(img, img_name, save_dir):\n",
    "    results = model(img)\n",
    "    \n",
    "    face_count = 0\n",
    "    for result in results:\n",
    "        # Obtener bounding boxes (person class = 0)\n",
    "        boxes = result.boxes.xyxy[result.boxes.cls == 0].cpu().numpy()\n",
    "        \n",
    "        for box in boxes:\n",
    "            xmin, ymin, xmax, ymax = map(int, box[:4])\n",
    "            face_crop = img[ymin:ymax, xmin:xmax]\n",
    "            face_count += 1\n",
    "            face_filename = save_dir / f\"{img_name}_face_{face_count}.jpg\"\n",
    "            cv2.imwrite(str(face_filename), cv2.cvtColor(face_crop, cv2.COLOR_RGB2BGR))\n",
    "            print(f\"Saved: {face_filename.name}\")\n",
    "    \n",
    "    # Save annotated image in output_dir_pose\n",
    "    for result in results:\n",
    "        annotated_img = result.plot()\n",
    "        annotated_path = output_dir_pose / f\"{img_name}_annotated.jpg\"\n",
    "        cv2.imwrite(str(annotated_path), cv2.cvtColor(annotated_img, cv2.COLOR_RGB2BGR))\n",
    "    print(f\"Annotated image saved: {annotated_path.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43cd6f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images():\n",
    "    for folder in input_dir.iterdir():\n",
    "        if not folder.is_dir():\n",
    "            continue\n",
    "        for img_path in folder.glob(\"*.jpg\"):\n",
    "            img = cv2.imread(str(img_path))\n",
    "            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            detect_faces_yolo_pose(img_rgb, img_path.stem, cropped_dir_pose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f0f35f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_to_images(pdf_path, dpi=300):\n",
    "    pdf_document = fitz.open(str(pdf_path))\n",
    "    img_paths = []\n",
    "    for page_num in range(len(pdf_document)):\n",
    "        page = pdf_document.load_page(page_num)\n",
    "        zoom = dpi / 72\n",
    "        mat = fitz.Matrix(zoom, zoom)\n",
    "        pix = page.get_pixmap(matrix=mat)\n",
    "        img_array = np.frombuffer(pix.samples, dtype=np.uint8).reshape((pix.height, pix.width, 3))\n",
    "        img_paths.append((img_array, page_num + 1))\n",
    "    return img_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0aae5b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images(max_images=25):\n",
    "    image_counter = 0\n",
    "    for folder in input_dir.iterdir():\n",
    "        if not folder.is_dir():\n",
    "            continue\n",
    "        for img_path in folder.glob(\"*.jpg\"):\n",
    "            if image_counter >= max_images:\n",
    "                print(f\"Reached the max limit of {max_images} images.\")\n",
    "                return\n",
    "            img = cv2.imread(str(img_path))\n",
    "            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            detect_faces_yolo_pose(img_rgb, img_path.stem, cropped_dir_pose)\n",
    "            image_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d92e3eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x384 (no detections), 57.0ms\n",
      "Speed: 3.7ms preprocess, 57.0ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Annotated image saved: 00_annotated.jpg\n",
      "\n",
      "0: 640x384 (no detections), 34.3ms\n",
      "Speed: 1.5ms preprocess, 34.3ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Annotated image saved: 01_annotated.jpg\n",
      "\n",
      "0: 640x384 (no detections), 36.0ms\n",
      "Speed: 1.7ms preprocess, 36.0ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Annotated image saved: 02_annotated.jpg\n",
      "\n",
      "0: 640x384 (no detections), 34.4ms\n",
      "Speed: 1.7ms preprocess, 34.4ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Annotated image saved: 03_annotated.jpg\n",
      "\n",
      "0: 640x384 1 person, 32.8ms\n",
      "Speed: 1.7ms preprocess, 32.8ms inference, 3.1ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Saved: 04_face_1.jpg\n",
      "Annotated image saved: 04_annotated.jpg\n",
      "\n",
      "0: 640x384 1 person, 35.7ms\n",
      "Speed: 1.8ms preprocess, 35.7ms inference, 2.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Saved: 05_face_1.jpg\n",
      "Annotated image saved: 05_annotated.jpg\n",
      "\n",
      "0: 640x384 (no detections), 34.6ms\n",
      "Speed: 1.8ms preprocess, 34.6ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Annotated image saved: 06_annotated.jpg\n",
      "\n",
      "0: 640x384 1 person, 34.8ms\n",
      "Speed: 2.0ms preprocess, 34.8ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Saved: 07_face_1.jpg\n",
      "Annotated image saved: 07_annotated.jpg\n",
      "\n",
      "0: 640x384 (no detections), 33.7ms\n",
      "Speed: 1.9ms preprocess, 33.7ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Annotated image saved: 08_annotated.jpg\n",
      "\n",
      "0: 640x384 2 persons, 33.2ms\n",
      "Speed: 1.6ms preprocess, 33.2ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Saved: 09_face_1.jpg\n",
      "Saved: 09_face_2.jpg\n",
      "Annotated image saved: 09_annotated.jpg\n",
      "\n",
      "0: 640x384 1 person, 35.5ms\n",
      "Speed: 1.9ms preprocess, 35.5ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Saved: 10_face_1.jpg\n",
      "Annotated image saved: 10_annotated.jpg\n",
      "\n",
      "0: 640x384 1 person, 35.1ms\n",
      "Speed: 1.8ms preprocess, 35.1ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Saved: 11_face_1.jpg\n",
      "Annotated image saved: 11_annotated.jpg\n",
      "\n",
      "0: 640x384 1 person, 37.5ms\n",
      "Speed: 1.9ms preprocess, 37.5ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Saved: 12_face_1.jpg\n",
      "Annotated image saved: 12_annotated.jpg\n",
      "\n",
      "0: 640x384 1 person, 41.5ms\n",
      "Speed: 1.9ms preprocess, 41.5ms inference, 3.4ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Saved: 13_face_1.jpg\n",
      "Annotated image saved: 13_annotated.jpg\n",
      "\n",
      "0: 640x384 3 persons, 40.6ms\n",
      "Speed: 2.0ms preprocess, 40.6ms inference, 3.2ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Saved: 14_face_1.jpg\n",
      "Saved: 14_face_2.jpg\n",
      "Saved: 14_face_3.jpg\n",
      "Annotated image saved: 14_annotated.jpg\n",
      "\n",
      "0: 640x384 (no detections), 36.6ms\n",
      "Speed: 1.7ms preprocess, 36.6ms inference, 2.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Annotated image saved: 15_annotated.jpg\n",
      "\n",
      "0: 640x384 1 person, 34.6ms\n",
      "Speed: 1.6ms preprocess, 34.6ms inference, 3.3ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Saved: 16_face_1.jpg\n",
      "Annotated image saved: 16_annotated.jpg\n",
      "\n",
      "0: 640x384 (no detections), 35.6ms\n",
      "Speed: 1.6ms preprocess, 35.6ms inference, 2.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Annotated image saved: 17_annotated.jpg\n",
      "\n",
      "0: 640x384 1 person, 37.3ms\n",
      "Speed: 2.2ms preprocess, 37.3ms inference, 3.1ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Saved: 18_face_1.jpg\n",
      "Annotated image saved: 18_annotated.jpg\n",
      "\n",
      "0: 640x384 1 person, 35.0ms\n",
      "Speed: 2.0ms preprocess, 35.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Saved: 19_face_1.jpg\n",
      "Annotated image saved: 19_annotated.jpg\n",
      "\n",
      "0: 640x384 1 person, 32.6ms\n",
      "Speed: 1.7ms preprocess, 32.6ms inference, 2.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Saved: 20_face_1.jpg\n",
      "Annotated image saved: 20_annotated.jpg\n",
      "\n",
      "0: 640x384 (no detections), 43.3ms\n",
      "Speed: 2.0ms preprocess, 43.3ms inference, 2.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Annotated image saved: 21_annotated.jpg\n",
      "\n",
      "0: 640x384 (no detections), 35.7ms\n",
      "Speed: 2.1ms preprocess, 35.7ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Annotated image saved: 22_annotated.jpg\n",
      "\n",
      "0: 640x384 2 persons, 39.5ms\n",
      "Speed: 2.2ms preprocess, 39.5ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Saved: 23_face_1.jpg\n",
      "Saved: 23_face_2.jpg\n",
      "Annotated image saved: 23_annotated.jpg\n",
      "\n",
      "0: 640x384 1 person, 37.7ms\n",
      "Speed: 2.1ms preprocess, 37.7ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Saved: 24_face_1.jpg\n",
      "Annotated image saved: 24_annotated.jpg\n",
      "Reached the max limit of 25 images.\n"
     ]
    }
   ],
   "source": [
    "process_images(max_images=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0fdb2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdfs():\n",
    "    pdf_files = list(pdf_dir.glob(\"*.pdf\"))\n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"Processing PDF: {pdf_file.name}\")\n",
    "        try:\n",
    "            pages = pdf_to_images(pdf_file)\n",
    "            for img_array, page_num in pages:\n",
    "                img_rgb = cv2.cvtColor(img_array, cv2.COLOR_BGR2RGB)\n",
    "                img_name = f\"{pdf_file.stem}_page_{page_num}\"\n",
    "                detect_faces_yolo_pose(img_rgb, img_name, pdf_cropped_dir)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {pdf_file.name}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2be45647",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_pdfs()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
